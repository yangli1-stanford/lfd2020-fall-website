# jemdoc: menu{MENU}{index.html},addcss{https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css},addcss{https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css},addjs{http://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.2.1.min},addjs{https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min}

== Learning from Data (Fall 2020)

Welcome to the class website of Learning from Data!

~~~
{Announcement}

*2020-09-28*: The make-up lecture for the National holiday will be held on Sunday, Sept 28th at 9:20am

*2020-09-09*: Since the first scheduled class conflicts with the Graduate School Admission Interview, the first class is switched to Sunday morning.
~~~


=== Class info

- Time: Friday 9:20-11:30
- Location: Info Building 510

For more information about grading, homework and exam policies, see the [./media/syllabus.pdf class syllabus].

==== Description
This introductory course gives an overview of many concepts, techniques, and algorithms in machine learning, beginning with topics such as logistic regression and SVM and ending up with more recent topics such as deep neural networks and reinforcement learning. The course will give the student the basic ideas and intuition behind modern machine learning methods as well as a bit more formal understanding of how, why, and when they work. The underlying theme in the course is statistical inference as it provides the foundation for most of the methods covered.

==== Prerequisites

Basic concepts in calculus, probability theory, and linear algebra.

==== Team
Instructor:
- Yang Li <yangli@sz.tsinghua.edu.cn>

TAs
- Weida Wang <wangwd19@mails.tsinghua.edu.cn>
- Feng Zhao <zhaof17@mails.tsinghua.edu.cn>

=== Office Hour

- Professor Yang: Friday 2:00-4:00pm
- TA Feng Zhao: Friday 8:00-10:00pm
- TA Weida Wang: Wednesday 6:00-8:00pm



=== Schedule
{{
<table class="table table-striped">
	<tr>
		<th>Date
		</th>
		<th>
			Topic
		</th>
		<th>
			Homework release
		</th>
	</tr>
	<tr>
		<td>
			9/18
		</td>
		<td>
			Review Session (optional)}} [media/review.pdf (notes)] [media/scientific_programming.pdf (slides)] [media/snippets.ipynb (code snippets)]
	{{ 	</td>
		<td>
			<a href="./media/HW0.pdf">WA0</a>&nbsp;
			<a href="./media/HW0_solution.pdf">WA0 solution</a>
		</td>
	</tr>
	<tr>
		<td style="color: red;">
			9/20 Sunday 9:20-12:00am
		</td>
		<td>
			Introduction (make up for 9/18) }}  [media/lecture1-handout.pdf (slides)]
	{{ </td>
		<td>
		</td>
	</tr>
	<tr>
		<td>
			9/25
		</td>
		<td>
			Supervised Learning I <br/> }}
			[media/lecture2-handout.pdf (slides)]
			[media/lecture2-annotated.pdf (slides with notes)]{{
		</td>
		<td>
			}}
			[media/pa1.pdf PA1]{{
		</td>
	</tr>	<tr>
		<td>
			9/28
		</td>
		<td>
			Supervised Learning II:<br/>
			Generalized linear model<br/>
			Model selection <br/>}}
			[media/lecture3.pdf (slides)]{{
		</td>
		<td>
			}}
			[media/WA1.pdf WA1]{{		
		</td>
	</tr>
	<tr>
		<td>
			10/09
		</td>
		<td>
			Supervised Learning III: <br/>
			Generative model: GDA <br/>
			Generative model: naive Bayesian model<br/>}}
			[media/lecture4-handout.pdf (slides)]
			[media/Compare_GDA_LR_v3.ipynb (code snippets)]{{
		</td>
		<td>
		</td>
	</tr>
	<!--
	<tr>
		<td>
			10/23
		</td>
		<td>
			Supervised Learning IV<br/>
			Support vector machines
		</td>
		<td>
			WA2
		</td>
	</tr>
	<tr>
		<td>
			10/30
		</td>
		<td>
			Supervised Learning V:<br/>
			Deep neural networks
		</td>
		<td>
			PA3
		</td>
	</tr>
	<tr>
		<td>
			11/6
		</td>
		<td>
			Midterm
		</td>
		<td>
		</td>
	</tr>
	<tr>
		<td>
			11/13
		</td>
		<td>
			Unsupervised Learning I:<br/>
			K-means clustering<br/>
			Principal component analysis<br/>
			Independent component analysis
		</td>
		<td>
			WA3
		</td>
	</tr>
	<tr>
		<td>
			11/20
		</td>
		<td>
			Unsupervised Learning II:<br/>
			Canonical component analysis<br/>
			Maximal HGR correlation
		</td>
		<td>
			PA4
		</td>
	</tr>
	<tr>
		<td>
			11/27
		</td>
		<td>
			Unsupervised Learning III:<br/>
			Mixture Gaussian and EM algorithm <br/>
			Spectral Clustering
		</td>
		<td>
			WA4, Final Project
		</td>
	</tr>
	<tr>
		<td>
			12/4
		</td>
		<td>
			Machine Learning Theory I <br/>
			Regularization <br/>
			Empirical risk, VC dimension
		</td>
		<td>
		</td>
	</tr>
	<tr>
		<td>
			12/11
		</td>
		<td>
			Machine Learning Theory II <br/>
			Hypothesis testing
		</td>
		<td>
		</td>
	</tr>
	<tr>
		<td>
			12/18
		</td>
		<td>
			Reinforcement Learning: <br/>
			Markov decision process <br/>
			Value iteration and policy iteration <br/>
			Q-Learning
		</td>
		<td>
			WA5
		</td>
	</tr>
	<tr>
		<td>
			12/25
		</td>
		<td>
			Advanced Topic I <br/>
			Transfer Learning
		</td>
		<td>
		</td>
	</tr>
	<tr>
		<td>
			1/1
		</td>
		<td>
			Advanced Topic II:<br/>
			Semi-supervised learning
		</td>
		<td>
		</td>
	</tr>
	<tr>
		<td>
			1/8
		</td>
		<td>
			Final Project Presentation I
		</td>
		<td>
		</td>
	</tr>
	<tr>
		<td>
			1/15
		</td>
		<td>
			Final Project Presentation II
		</td>
		<td>
		</td>
	</tr>-->
</table>
}}
