# jemdoc: menu{MENU}{index.html},addcss{https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css},addcss{https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css},addjs{http://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.2.1.min},addjs{https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min}

== Learning from Data (Fall 2020)

Welcome to the class website of Learning from Data!

~~~
{Announcement}

*2020-12-3*: The guest lecture, given by Pedro Baiz, will be given from 10:00am - 11:00 am at Info Building rom 510.

*2020-10-29*: The midterm exam will be held in class on Friday, November 6th.
 For anyone who can not attend the in-class exam, please contact a TA to fill out the online exam application before Nov 3rd.

*2020-09-28*: The make-up lecture for the National holiday will be held on Sunday, Sept 28th at 9:20am

*2020-09-09*: Since the first scheduled class conflicts with the Graduate School Admission Interview, the first class is switched to Sunday morning.
~~~


=== Class info

- Time: Friday 8:50-11:25
- Location: Info Building 510

For more information about grading, homework and exam policies, see the [./media/syllabus.pdf class syllabus].

==== Description
This introductory course gives an overview of many concepts, techniques, and algorithms in machine learning, beginning with topics such as logistic regression and SVM and ending up with more recent topics such as deep neural networks and reinforcement learning. The course will give the student the basic ideas and intuition behind modern machine learning methods as well as a bit more formal understanding of how, why, and when they work. The underlying theme in the course is statistical inference as it provides the foundation for most of the methods covered.

==== Prerequisites

Basic concepts in calculus, probability theory, and linear algebra.

==== Team
Instructor:
- Yang Li <yangli@sz.tsinghua.edu.cn>

TAs
- Weida Wang <wangwd19@mails.tsinghua.edu.cn>
- Feng Zhao <zhaof17@mails.tsinghua.edu.cn>

=== Office Hour

- Professor Yang: Friday 2:00-4:00pm
- TA Feng Zhao: Friday 8:00-10:00pm
- TA Weida Wang: Wednesday 6:00-8:00pm



=== Schedule
{{
<table class="table table-striped">
	<tr>
		<th>Date
		</th>
		<th>
			Topic
		</th>
		<th>
			Homework release
		</th>
	</tr>
	<tr>
		<td>
			9/18
		</td>
		<td>
			Review Session (optional) <br/>}}
			[media/review.pdf (notes)] |
			[media/scientific_programming.pdf (slides)] |
			[media/snippets.ipynb (code snippets)]
	{{ 	</td>
		<td>
		}}
			[media/HW0.pdf WA0] |
				[media/HW0_solution.pdf WA0 solution]
			{{
		</td>
	</tr>
	<tr>
		<td style="color: red;">
			9/20 Sunday 9:20-12:00am
		</td>
		<td>
			Introduction (make up for 9/18) }}  [media/lecture1-handout.pdf (slides)]
	{{ </td>
		<td>
		</td>
	</tr>
	<tr>
		<td>
			9/25
		</td>
		<td>
			Supervised Learning I <br/> }}
			[media/lecture2-handout.pdf (slides)] |
			[media/lecture2-annotated.pdf (slides with notes)]{{
		</td>
		<td>
			}}
			[media/pa1.pdf PA1]{{
		</td>
	</tr>	<tr>
		<td>
			9/28
		</td>
		<td>
			Supervised Learning II:<br/>
			Generalized linear model<br/>
			Model selection <br/>}}
			[media/lecture3.pdf (slides)] |
			[media/lecture3-annotated.pdf (slides with notes)]{{
		</td>
		<td>
			}}
			[media/WA1.pdf WA1] |
			[media/WA1-solution.pdf WA1 solution]{{
		</td>
	</tr>
	<tr>
		<td>
			10/09
		</td>
		<td>
			Supervised Learning III: <br/>
			Generative model: GDA <br/>
			Generative model: naive Bayesian model<br/>}}
			[media/lecture4-handout.pdf (slides)] |
			[media/lecture4-annotated.pdf (slides with notes)] |
			[media/Compare_GDA_LR_v3.ipynb (code snippets)]{{
		</td>
		<td>
		</td>
	</tr>
	<tr>
		<td>
			10/16
		</td>
		<td>
			Supervised Learning IV<br/>
			Support vector machines<br/>}}
			[media/lecture5-handout.pdf (slides)] |
			[media/lecture5-annotated.pdf (slides with notes)]{{
		</td>
		<td>}}
			[media/WA2.pdf WA2] |
			[media/WA2-solution.pdf WA2 solution]{{
		</td>
	</tr>
	<tr>
		<td>
			10/23
		</td>
		<td>
			Supervised Learning V:<br/>
			Deep neural networks }}
			[media/lecture6-handout.pdf (slides \#1)] |
			[media/lecture6-part2.pptx (slides \#2)]{{
		</td>
		<td>
			}}
			[media/PA2.pdf PA2] |
			[media/PA2_slides.pdf PA2 tutorial]{{
		</td>
	</tr>
	<tr>
		<td>
			10/30
		</td>
		<td>
			Unsupervised Learning I:<br/>
			K-means clustering<br/>
			Principal component analysis<br/>}}
			[media/lecture7-handout.pdf (slides)]{{
		</td>
		<td>
		</td>
	</tr>
	<tr>
		<td>
			11/6
		</td>
		<td>
			Midterm Exam
		</td>
		<td>
		</td>
	</tr>
	<tr>
		<td>
			11/13
		</td>
		<td>
			Unsupervised Learning II:<br/>
			Independent component analysis<br/>
			Canonical component analysis<br/>}}
			[media/lecture8-handout.pdf (slides)]{{
		</td>
		<td>
						}}
			[media/WA3.pdf WA3]{{
		</td>
	</tr>
	<tr>
		<td>
			11/20
		</td>
		<td>
			Unsupervised Learning III:<br/>
			Maximal HGR correlation<br/>
			Spectral Clustering}}
			[media/lecture9-handout.pdf (slides)]{{
		</td>
		<td>
						}}
			[media/PA3.pdf PA3]{{
		</td>
	</tr>	
	<tr>
		<td>
			11/27
		</td>
		<td>
			Unsupervised Learning IV:<br/>
			Mixture Gaussian and EM algorithm <br/>
			Factor Analysis<br/>}}
			[media/lecture10-handout.pdf (slides)]{{
		</td>
		<td>
						}}		
			[media/WA4.pdf WA4]{{		
		</td>
	</tr>
	<tr>
		<td>
			12/11
		</td>
		<td>
			Reinforcement Learning: <br/>
			Markov decision process <br/>
			Value iteration and policy iteration <br/>
			Q-Learning<br/>}}
			[media/lecture11-handout.pdf (slides)]{{
		</td>
		<td>
			WA5
		</td>
	</tr>	
	<!--
	<tr>
		<td>
			12/4
		</td>
		<td>
			Machine Learning Theory II <br/>
			Hypothesis testing
		</td>
		<td>
		</td>
	</tr>
	<tr>
		<td>
			12/18
		</td>
		<td>
			Advanced Topic I <br/>
			Transfer Learning
		</td>
		<td>
		</td>
	</tr>
	<tr>
		<td>
			12/25
		</td>
		<td>
			Advanced Topic II:<br/>
			Semi-supervised learning
		</td>
		<td>
		</td>
	</tr>
	<tr>
		<td>
			1/1
		</td>
		<td>
			Final Project Presentation I
		</td>
		<td>
		</td>
	</tr>
	<tr>
		<td>
			1/15
		</td>
		<td>
			Final Project Presentation II
		</td>
		<td>
		</td>
	</tr>-->
</table>
}}
